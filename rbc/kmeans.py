import math
import os
import glob

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tslearn.clustering import TimeSeriesKMeans
from tslearn.preprocessing import TimeSeriesScalerMeanVariance

"""
Adaptation of demo script by: Romain Tavenard
License: BSD 3 clause
https://tslearn.readthedocs.io/en/stable/auto_examples/clustering/plot_kmeans.html#sphx-glr-auto-examples-clustering-plot-kmeans-py

explanation of some important variables

time_series:
a numpy array where each row is a time series for a single point and there is 1 row for each series

assigned_clusters:
is the same as km.labels_ and shows the cluster number that each row was assigned. Thus it is a list of the same length 
you provided number of input features/rows

km:
is the kmeans model itself. the valuable information stored in it is the location of the centroid for each cluster. 
the centroids are generated by the 'fit' option and used to 'predict' new matches. they are modified on the fly and 
used in prediction when you call 'fit_predict'

if all we were interested in were the locations of the centroids then we would shuffle the data before putting it into
the clustering algorithm but we need the data to not be shuffled so that we can pair the info against the ordered comids
and know which comid was put into each cluster

n_clusters = km.n_clusters

You want to run trials of lots of numbers of clusters and figure out what minimum number of clusters will characterize 
the trends in the data and use that. Doing more makes finding strong observational data matches more difficult
"""


def generate_clusters(workdir: str, num_clusters: list = range(4, 13)):
    """
    Creates trained kmeans model pickle files and plots of the results saved as png images

    Args:
        workdir: path to the project directory
        num_clusters: an iterable of integers, the number of kmeans clusters to create.

    Returns:
        None
    """
    tables_to_cluster = glob.glob(os.path.join(workdir, 'data_*', '*.csv'))

    for table in tables_to_cluster:
        # read the data
        time_series = pd.read_csv(table, index_col=0).dropna(axis=1)
        time_series = np.transpose(time_series.values)
        dataset = os.path.basename(table)

        for num_cluster in num_clusters:
            km = TimeSeriesKMeans(n_clusters=num_cluster, verbose=True, random_state=0)
            km.fit_predict(TimeSeriesScalerMeanVariance().fit_transform(time_series))

            # save the trained model
            km.to_pickle(os.path.join(workdir, 'kmeans_models', f'{dataset}-{num_cluster}-clusters-model.pickle'))

            size = time_series.shape[1]
            fig = plt.figure(figsize=(30, 15), dpi=450)
            assigned_clusters = km.labels_
            for i in range(num_cluster):
                plt.subplot(2, math.ceil(num_cluster / 2), i + 1)
                for j in time_series[assigned_clusters == i]:
                    plt.plot(j.ravel(), "k-", alpha=.2)
                plt.plot(km.cluster_centers_[i].ravel(), "r-")
                plt.xlim(0, size)
                plt.ylim(0, np.max(time_series))
                plt.text(0.55, 0.85, f'Cluster {i}', transform=plt.gca().transAxes)
                if i == math.floor(num_cluster / 4):
                    plt.title("Euclidean $k$-means")

            plt.tight_layout()
            fig.savefig(os.path.join(workdir, 'kmeans_images', f'{dataset}-{num_cluster}-clusters.png'))
            plt.close(fig)
    return
