import os
import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tslearn.clustering import TimeSeriesKMeans
from tslearn.preprocessing import TimeSeriesScalerMeanVariance

"""
Adaptation of demo script by: Romain Tavenard
License: BSD 3 clause
https://tslearn.readthedocs.io/en/stable/auto_examples/clustering/plot_kmeans.html#sphx-glr-auto-examples-clustering-plot-kmeans-py

explanation of some important variables

time_series:
a numpy array where each row is a time series for a single point and there is 1 row for each series

assigned_clusters:
is the same as km.labels_ and shows the cluster number that each row was assigned. Thus it is a list of the same length 
you provided number of input features/rows

km:
is the kmeans model itself. the valuable information stored in it is the location of the centroid for each cluster. 
the centroids are generated by the 'fit' option and used to 'predict' new matches. they are modified on the fly and 
used in prediction when you call 'fit_predict'

if all we were interested in were the locations of the centroids then we would shuffle the data before putting it into
the clustering algorithm but we need the data to not be shuffled so that we can pair the info against the ordered comids
and know which comid was put into each cluster

n_clusters = km.n_clusters

You want to run trials of lots of numbers of clusters and figure out what minimum number of clusters will characterize 
the trends in the data and use that. Doing more makes finding strong observational data matches more difficult
"""


def plot_clusters(series: np.array, km: TimeSeriesKMeans, save_dir: str, name: str):
    size = series.shape[1]
    fig = plt.figure(figsize=(30, 15), dpi=450)
    n_clusters = km.n_clusters
    assigned_clusters = km.labels_
    for yi in range(n_clusters):
        plt.subplot(2, math.ceil(n_clusters / 2), yi + 1)
        for xx in series[assigned_clusters == yi]:
            plt.plot(xx.ravel(), "k-", alpha=.2)
        plt.plot(km.cluster_centers_[yi].ravel(), "r-")
        plt.xlim(0, size)
        plt.ylim(0, np.max(series))
        plt.text(0.55, 0.85, 'Cluster %d' % (yi + 1), transform=plt.gca().transAxes)
        if yi == math.floor(n_clusters / 4):
            plt.title("Euclidean $k$-means")

    plt.tight_layout()
    fig.savefig(os.path.join(save_dir, f'{name}_{n_clusters}cluster.png'))
    return


model_dir = '/Users/riley/code/basin_matching/data_2_cluster_models'
save_dir = '/Users/riley/code/basin_matching/data_2_cluster_images'


def generate_clusters(table_path: str, dataset: str, save_path: str, num_clusters: list = range(4, 13)):
    for num_cluster in num_clusters:
        # read the data
        # time_series = pd.read_csv('../data_1_historical_csv/observed_fdc.csv', index_col=0).dropna(axis=1)
        time_series = pd.read_csv(table_path, index_col=0).dropna(axis=1)
        time_series = np.transpose(time_series.values)

        # time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)
        km = TimeSeriesKMeans(n_clusters=num_cluster, verbose=True, random_state=0)
        km.fit_predict(TimeSeriesScalerMeanVariance().fit_transform(time_series))

        # save the trained model
        km.to_pickle(os.path.join(save_path, f'{dataset}_{num_cluster}_cluster_model.pickle'))

        plot_clusters(time_series, km, save_dir, dataset)


# print('starting sim_fdc')
# # fit the simulated fdc groups
# time_series = pd.read_csv('data_1_historical_csv/simulated_fdc_normalized.csv', index_col=0).dropna(axis=1)
# time_series = np.transpose(time_series.values)
# time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)
# km = fit_kmeans_clusters(time_series, 'sim_fdc', model_dir, clusters)
# plot_clusters(time_series, km, model_dir, 'sim_fdc')
# print('starting sim_monavg')
# # fit the simulated monthly average (seasonality) groups
# time_series = pd.read_csv('data_1_historical_csv/simulated_monavg_normalized.csv', index_col=0).dropna(axis=1)
# time_series = np.transpose(time_series.values)
# time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)
# km = fit_kmeans_clusters(time_series, 'sim_monavg', model_dir, clusters)
# plot_clusters(time_series, km, model_dir, 'sim_monavg')

# for clusters in range(4, 13):
#     # predict the observational fdc groups
#     print('starting obs_fdc')
#     name = 'obs_fdc_unnormalized_nomeanvariance'
#     time_series = pd.read_csv('../data_1_historical_csv/observed_fdc.csv', index_col=0).dropna(axis=1)
#     time_series = np.transpose(time_series.values)
#     # time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)
#     km = fit_kmeans_clusters(time_series, name, clusters)
#     km.to_pickle(f'data_2_cluster_models/{dtype}_fdc_{clusters}cluster_model.pickle')
#     plot_clusters(time_series, km, save_dir, name)
#
#     # predict the observational monthly average (seasonality) groups
#     print('starting obs_monavg')
#     name = 'obs_monavg_unnormalized_nomeanvariance'
#     time_series = pd.read_csv('../data_1_historical_csv/observed_monavg.csv', index_col=0).dropna(axis=1)
#     time_series = np.transpose(time_series.values)
#     # time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)
#     km = fit_kmeans_clusters(time_series, name, model_dir, clusters)
#     km.to_pickle(f'data_2_cluster_models/{dtype}_monavg_{clusters}cluster_model.pickle')
#     plot_clusters(time_series, km, save_dir, name)
